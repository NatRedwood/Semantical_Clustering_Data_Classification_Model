{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import importlib, import_ipynb\n",
    "import data_clean_order as data\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import experimental\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# model building imports\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# matplotlib defaults\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ordered_super_alpha_text\n",
    "%store -r ordered_class_alpha_text\n",
    "%store -r ordered_sem_clusters_desc_text\n",
    "%store -r ordered_sem_clusters_asc_text\n",
    "%store -r ordered_sem_clusters_shuffled_per_superclass_text\n",
    "%store -r test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental data orderings on headlines TRAIN EXAMPLES\n",
    "X1_train = ordered_super_alpha_text['cleaned_headline']\n",
    "X2_train = ordered_class_alpha_text['cleaned_headline']\n",
    "X3_train = ordered_sem_clusters_desc_text['cleaned_headline']\n",
    "X4_train = ordered_sem_clusters_asc_text['cleaned_headline']\n",
    "X5_train = ordered_sem_clusters_shuffled_per_superclass_text['cleaned_headline']\n",
    "\n",
    "# Experimental data orderings on short_description #TODO\n",
    "\n",
    "# TRAIN LABELS\n",
    "Y1_train = ordered_super_alpha_text['superclass']\n",
    "Y2_train = ordered_class_alpha_text['superclass']\n",
    "Y3_train = ordered_sem_clusters_desc_text['superclass']\n",
    "Y4_train = ordered_sem_clusters_asc_text['superclass']\n",
    "Y5_train = ordered_sem_clusters_shuffled_per_superclass_text['superclass']\n",
    "\n",
    "# TEST EXAMPLES\n",
    "X_test = test_df['cleaned_headline']\n",
    "Y_test = test_df['superclass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### One-hot encoding and indexing of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data:  (41905,)\n",
      "shape of target variable:  (41905,)\n",
      "Length of word index: 27262\n"
     ]
    }
   ],
   "source": [
    "# TEST DATA\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_test = encoder.fit_transform(Y_test)\n",
    "print(\"shape of input data: \", X_test.shape)\n",
    "print(\"shape of target variable: \", Y_test.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_test) # build the word index\n",
    "# padding X_test text input data\n",
    "test_seq = tokenizer.texts_to_sequences(X_test) # converts strinfs into integer lists\n",
    "test_padseq = pad_sequences(test_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 150000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index)\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_test = to_categorical(Y_test, num_classes=9)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data:  (167616,)\n",
      "shape of target variable:  (167616,)\n",
      "Length of word index: 53645\n"
     ]
    }
   ],
   "source": [
    "# ORDER 1 TRAIN DATA\n",
    "X_train = X1_train\n",
    "Y_train = Y1_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=9)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 100)           5364500   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 20, 128)          84480     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 20, 128)          98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 20, 128)          24704     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 18, 72)            27720     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 9, 72)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 9, 64)             8768      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,634,533\n",
      "Trainable params: 5,634,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(9, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1310/1310 [==============================] - 352s 243ms/step - loss: 1.6846 - accuracy: 0.5364\n",
      "Epoch 2/15\n",
      "1310/1310 [==============================] - 320s 244ms/step - loss: 1.1254 - accuracy: 0.6628\n",
      "Epoch 3/15\n",
      "1310/1310 [==============================] - 324s 247ms/step - loss: 1.0159 - accuracy: 0.6999\n",
      "Epoch 4/15\n",
      "1310/1310 [==============================] - 317s 242ms/step - loss: 0.9428 - accuracy: 0.7238 - loss: 0.9425 - accuracy: \n",
      "Epoch 5/15\n",
      "1310/1310 [==============================] - 304s 232ms/step - loss: 0.8821 - accuracy: 0.7434\n",
      "Epoch 6/15\n",
      "1310/1310 [==============================] - 314s 240ms/step - loss: 0.8316 - accuracy: 0.7605\n",
      "Epoch 7/15\n",
      "1310/1310 [==============================] - 324s 247ms/step - loss: 0.7891 - accuracy: 0.7749\n",
      "Epoch 8/15\n",
      "1310/1310 [==============================] - 318s 242ms/step - loss: 0.7531 - accuracy: 0.7877\n",
      "Epoch 9/15\n",
      "1310/1310 [==============================] - 311s 237ms/step - loss: 0.7202 - accuracy: 0.7985\n",
      "Epoch 10/15\n",
      "1310/1310 [==============================] - 305s 233ms/step - loss: 0.6921 - accuracy: 0.8077\n",
      "Epoch 11/15\n",
      "1310/1310 [==============================] - 317s 242ms/step - loss: 0.6668 - accuracy: 0.8163\n",
      "Epoch 12/15\n",
      "1310/1310 [==============================] - 311s 237ms/step - loss: 0.6445 - accuracy: 0.8247\n",
      "Epoch 13/15\n",
      "1310/1310 [==============================] - 325s 248ms/step - loss: 0.6252 - accuracy: 0.8310\n",
      "Epoch 14/15\n",
      "1310/1310 [==============================] - 327s 250ms/step - loss: 0.6065 - accuracy: 0.8355\n",
      "Epoch 15/15\n",
      "1310/1310 [==============================] - 327s 250ms/step - loss: 0.5907 - accuracy: 0.8412\n",
      "test loss and accuracy: 3.3512141704559326 0.20314998924732208\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, Y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, Y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "\n",
    "    # create object of arrays of accuracy and loss\n",
    "    acc = history.history['accuracy']\n",
    "    #val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    #val_loss = history.history['val_loss']\n",
    "    \n",
    "    # number of epochs in our model\n",
    "    epochs = range(1 ,len(acc) + 1)\n",
    "    \n",
    "    # call matplolib figure object and plot loss and accuracy curves\n",
    "    plt.figure(figsize=(15,6))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    #plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title(\"Training accuracy\", fontsize=15)\n",
    "    plt.xlabel('epochs', fontsize=14)\n",
    "    plt.ylabel(\"accuracy\", fontsize=14)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    #plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title(\"Training loss\", fontsize=15)\n",
    "    plt.xlabel('epochs', fontsize=14)\n",
    "    plt.ylabel(\"loss\", fontsize=14)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1072.8pt\" height=\"424.8pt\" viewBox=\"0 0 1072.8 424.8\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-12-13T14:09:41.737809</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 424.8 \nL 1072.8 424.8 \nL 1072.8 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.444688 384.670937 \nL 533.780469 384.670937 \nL 533.780469 27.936719 \nL 50.444688 27.936719 \nz\n\" style=\"fill: #e5e5e5\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 103.799936 384.670937 \nL 103.799936 27.936719 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 2 -->\n      <g style=\"fill: #555555\" transform=\"translate(101.019467 398.82875)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-32\" d=\"M 3222 541 \nL 3222 0 \nL 194 0 \nQ 188 203 259 391 \nQ 375 700 629 1000 \nQ 884 1300 1366 1694 \nQ 2113 2306 2375 2664 \nQ 2638 3022 2638 3341 \nQ 2638 3675 2398 3904 \nQ 2159 4134 1775 4134 \nQ 1369 4134 1125 3890 \nQ 881 3647 878 3216 \nL 300 3275 \nQ 359 3922 746 4261 \nQ 1134 4600 1788 4600 \nQ 2447 4600 2831 4234 \nQ 3216 3869 3216 3328 \nQ 3216 3053 3103 2787 \nQ 2991 2522 2730 2228 \nQ 2469 1934 1863 1422 \nQ 1356 997 1212 845 \nQ 1069 694 975 541 \nL 3222 541 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path d=\"M 166.570817 384.670937 \nL 166.570817 27.936719 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 4 -->\n      <g style=\"fill: #555555\" transform=\"translate(163.790348 398.82875)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-34\" d=\"M 2069 0 \nL 2069 1097 \nL 81 1097 \nL 81 1613 \nL 2172 4581 \nL 2631 4581 \nL 2631 1613 \nL 3250 1613 \nL 3250 1097 \nL 2631 1097 \nL 2631 0 \nL 2069 0 \nz\nM 2069 1613 \nL 2069 3678 \nL 634 1613 \nL 2069 1613 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path d=\"M 229.341697 384.670937 \nL 229.341697 27.936719 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 6 -->\n      <g style=\"fill: #555555\" transform=\"translate(226.561229 398.82875)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-36\" d=\"M 3184 3459 \nL 2625 3416 \nQ 2550 3747 2413 3897 \nQ 2184 4138 1850 4138 \nQ 1581 4138 1378 3988 \nQ 1113 3794 959 3422 \nQ 806 3050 800 2363 \nQ 1003 2672 1297 2822 \nQ 1591 2972 1913 2972 \nQ 2475 2972 2870 2558 \nQ 3266 2144 3266 1488 \nQ 3266 1056 3080 686 \nQ 2894 316 2569 119 \nQ 2244 -78 1831 -78 \nQ 1128 -78 684 439 \nQ 241 956 241 2144 \nQ 241 3472 731 4075 \nQ 1159 4600 1884 4600 \nQ 2425 4600 2770 4297 \nQ 3116 3994 3184 3459 \nz\nM 888 1484 \nQ 888 1194 1011 928 \nQ 1134 663 1356 523 \nQ 1578 384 1822 384 \nQ 2178 384 2434 671 \nQ 2691 959 2691 1453 \nQ 2691 1928 2437 2201 \nQ 2184 2475 1800 2475 \nQ 1419 2475 1153 2201 \nQ 888 1928 888 1484 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path d=\"M 292.112578 384.670937 \nL 292.112578 27.936719 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 8 -->\n      <g style=\"fill: #555555\" transform=\"translate(289.332109 398.82875)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-38\" d=\"M 1131 2484 \nQ 781 2613 612 2850 \nQ 444 3088 444 3419 \nQ 444 3919 803 4259 \nQ 1163 4600 1759 4600 \nQ 2359 4600 2725 4251 \nQ 3091 3903 3091 3403 \nQ 3091 3084 2923 2848 \nQ 2756 2613 2416 2484 \nQ 2838 2347 3058 2040 \nQ 3278 1734 3278 1309 \nQ 3278 722 2862 322 \nQ 2447 -78 1769 -78 \nQ 1091 -78 675 323 \nQ 259 725 259 1325 \nQ 259 1772 486 2073 \nQ 713 2375 1131 2484 \nz\nM 1019 3438 \nQ 1019 3113 1228 2906 \nQ 1438 2700 1772 2700 \nQ 2097 2700 2305 2904 \nQ 2513 3109 2513 3406 \nQ 2513 3716 2298 3927 \nQ 2084 4138 1766 4138 \nQ 1444 4138 1231 3931 \nQ 1019 3725 1019 3438 \nz\nM 838 1322 \nQ 838 1081 952 856 \nQ 1066 631 1291 507 \nQ 1516 384 1775 384 \nQ 2178 384 2440 643 \nQ 2703 903 2703 1303 \nQ 2703 1709 2433 1975 \nQ 2163 2241 1756 2241 \nQ 1359 2241 1098 1978 \nQ 838 1716 838 1322 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path d=\"M 354.883459 384.670937 \nL 354.883459 27.936719 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10 -->\n      <g style=\"fill: #555555\" transform=\"translate(349.322521 398.82875)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-31\" d=\"M 2384 0 \nL 1822 0 \nL 1822 3584 \nQ 1619 3391 1289 3197 \nQ 959 3003 697 2906 \nL 697 3450 \nQ 1169 3672 1522 3987 \nQ 1875 4303 2022 4600 \nL 2384 4600 \nL 2384 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"ArialMT-30\" d=\"M 266 2259 \nQ 266 3072 433 3567 \nQ 600 4063 929 4331 \nQ 1259 4600 1759 4600 \nQ 2128 4600 2406 4451 \nQ 2684 4303 2865 4023 \nQ 3047 3744 3150 3342 \nQ 3253 2941 3253 2259 \nQ 3253 1453 3087 958 \nQ 2922 463 2592 192 \nQ 2263 -78 1759 -78 \nQ 1097 -78 719 397 \nQ 266 969 266 2259 \nz\nM 844 2259 \nQ 844 1131 1108 757 \nQ 1372 384 1759 384 \nQ 2147 384 2411 759 \nQ 2675 1134 2675 2259 \nQ 2675 3391 2411 3762 \nQ 2147 4134 1753 4134 \nQ 1366 4134 1134 3806 \nQ 844 3388 844 2259 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path d=\"M 417.654339 384.670937 \nL 417.654339 27.936719 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12 -->\n      <g style=\"fill: #555555\" transform=\"translate(412.093402 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-32\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path d=\"M 480.42522 384.670937 \nL 480.42522 27.936719 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 14 -->\n      <g style=\"fill: #555555\" transform=\"translate(474.864283 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-34\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- epochs -->\n     <g style=\"fill: #555555\" transform=\"translate(267.608203 414.837187)scale(0.14 -0.14)\">\n      <defs>\n       <path id=\"Arial-BoldMT-65\" d=\"M 2381 1056 \nL 3256 909 \nQ 3088 428 2723 176 \nQ 2359 -75 1813 -75 \nQ 947 -75 531 491 \nQ 203 944 203 1634 \nQ 203 2459 634 2926 \nQ 1066 3394 1725 3394 \nQ 2466 3394 2894 2905 \nQ 3322 2416 3303 1406 \nL 1103 1406 \nQ 1113 1016 1316 798 \nQ 1519 581 1822 581 \nQ 2028 581 2168 693 \nQ 2309 806 2381 1056 \nz\nM 2431 1944 \nQ 2422 2325 2234 2523 \nQ 2047 2722 1778 2722 \nQ 1491 2722 1303 2513 \nQ 1116 2303 1119 1944 \nL 2431 1944 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"Arial-BoldMT-70\" d=\"M 434 3319 \nL 1253 3319 \nL 1253 2831 \nQ 1413 3081 1684 3237 \nQ 1956 3394 2288 3394 \nQ 2866 3394 3269 2941 \nQ 3672 2488 3672 1678 \nQ 3672 847 3265 386 \nQ 2859 -75 2281 -75 \nQ 2006 -75 1782 34 \nQ 1559 144 1313 409 \nL 1313 -1263 \nL 434 -1263 \nL 434 3319 \nz\nM 1303 1716 \nQ 1303 1156 1525 889 \nQ 1747 622 2066 622 \nQ 2372 622 2575 867 \nQ 2778 1113 2778 1672 \nQ 2778 2194 2568 2447 \nQ 2359 2700 2050 2700 \nQ 1728 2700 1515 2451 \nQ 1303 2203 1303 1716 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"Arial-BoldMT-6f\" d=\"M 256 1706 \nQ 256 2144 472 2553 \nQ 688 2963 1083 3178 \nQ 1478 3394 1966 3394 \nQ 2719 3394 3200 2905 \nQ 3681 2416 3681 1669 \nQ 3681 916 3195 420 \nQ 2709 -75 1972 -75 \nQ 1516 -75 1102 131 \nQ 688 338 472 736 \nQ 256 1134 256 1706 \nz\nM 1156 1659 \nQ 1156 1166 1390 903 \nQ 1625 641 1969 641 \nQ 2313 641 2545 903 \nQ 2778 1166 2778 1666 \nQ 2778 2153 2545 2415 \nQ 2313 2678 1969 2678 \nQ 1625 2678 1390 2415 \nQ 1156 2153 1156 1659 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"Arial-BoldMT-63\" d=\"M 3353 2338 \nL 2488 2181 \nQ 2444 2441 2289 2572 \nQ 2134 2703 1888 2703 \nQ 1559 2703 1364 2476 \nQ 1169 2250 1169 1719 \nQ 1169 1128 1367 884 \nQ 1566 641 1900 641 \nQ 2150 641 2309 783 \nQ 2469 925 2534 1272 \nL 3397 1125 \nQ 3263 531 2881 228 \nQ 2500 -75 1859 -75 \nQ 1131 -75 698 384 \nQ 266 844 266 1656 \nQ 266 2478 700 2936 \nQ 1134 3394 1875 3394 \nQ 2481 3394 2839 3133 \nQ 3197 2872 3353 2338 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"Arial-BoldMT-68\" d=\"M 1334 4581 \nL 1334 2897 \nQ 1759 3394 2350 3394 \nQ 2653 3394 2897 3281 \nQ 3141 3169 3264 2994 \nQ 3388 2819 3433 2606 \nQ 3478 2394 3478 1947 \nL 3478 0 \nL 2600 0 \nL 2600 1753 \nQ 2600 2275 2550 2415 \nQ 2500 2556 2373 2639 \nQ 2247 2722 2056 2722 \nQ 1838 2722 1666 2615 \nQ 1494 2509 1414 2295 \nQ 1334 2081 1334 1663 \nL 1334 0 \nL 456 0 \nL 456 4581 \nL 1334 4581 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"Arial-BoldMT-73\" d=\"M 150 947 \nL 1031 1081 \nQ 1088 825 1259 692 \nQ 1431 559 1741 559 \nQ 2081 559 2253 684 \nQ 2369 772 2369 919 \nQ 2369 1019 2306 1084 \nQ 2241 1147 2013 1200 \nQ 950 1434 666 1628 \nQ 272 1897 272 2375 \nQ 272 2806 612 3100 \nQ 953 3394 1669 3394 \nQ 2350 3394 2681 3172 \nQ 3013 2950 3138 2516 \nL 2309 2363 \nQ 2256 2556 2107 2659 \nQ 1959 2763 1684 2763 \nQ 1338 2763 1188 2666 \nQ 1088 2597 1088 2488 \nQ 1088 2394 1175 2328 \nQ 1294 2241 1995 2081 \nQ 2697 1922 2975 1691 \nQ 3250 1456 3250 1038 \nQ 3250 581 2869 253 \nQ 2488 -75 1741 -75 \nQ 1063 -75 667 200 \nQ 272 475 150 947 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#Arial-BoldMT-65\"/>\n      <use xlink:href=\"#Arial-BoldMT-70\" x=\"55.615234\"/>\n      <use xlink:href=\"#Arial-BoldMT-6f\" x=\"116.699219\"/>\n      <use xlink:href=\"#Arial-BoldMT-63\" x=\"177.783203\"/>\n      <use xlink:href=\"#Arial-BoldMT-68\" x=\"233.398438\"/>\n      <use xlink:href=\"#Arial-BoldMT-73\" x=\"294.482422\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <path d=\"M 50.444688 353.983675 \nL 533.780469 353.983675 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.55 -->\n      <g style=\"fill: #555555\" transform=\"translate(23.98375 357.562582)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-2e\" d=\"M 581 0 \nL 581 641 \nL 1222 641 \nL 1222 0 \nL 581 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"ArialMT-35\" d=\"M 266 1200 \nL 856 1250 \nQ 922 819 1161 601 \nQ 1400 384 1738 384 \nQ 2144 384 2425 690 \nQ 2706 997 2706 1503 \nQ 2706 1984 2436 2262 \nQ 2166 2541 1728 2541 \nQ 1456 2541 1237 2417 \nQ 1019 2294 894 2097 \nL 366 2166 \nL 809 4519 \nL 3088 4519 \nL 3088 3981 \nL 1259 3981 \nL 1013 2750 \nQ 1425 3038 1878 3038 \nQ 2478 3038 2890 2622 \nQ 3303 2206 3303 1553 \nQ 3303 931 2941 478 \nQ 2500 -78 1738 -78 \nQ 1113 -78 717 272 \nQ 322 622 266 1200 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-35\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-35\" x=\"139.013672\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <path d=\"M 50.444688 300.782818 \nL 533.780469 300.782818 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.60 -->\n      <g style=\"fill: #555555\" transform=\"translate(23.98375 304.361725)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-36\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <path d=\"M 50.444688 247.581961 \nL 533.780469 247.581961 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.65 -->\n      <g style=\"fill: #555555\" transform=\"translate(23.98375 251.160867)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-36\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-35\" x=\"139.013672\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <path d=\"M 50.444688 194.381104 \nL 533.780469 194.381104 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.70 -->\n      <g style=\"fill: #555555\" transform=\"translate(23.98375 197.96001)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-37\" d=\"M 303 3981 \nL 303 4522 \nL 3269 4522 \nL 3269 4084 \nQ 2831 3619 2401 2847 \nQ 1972 2075 1738 1259 \nQ 1569 684 1522 0 \nL 944 0 \nQ 953 541 1156 1306 \nQ 1359 2072 1739 2783 \nQ 2119 3494 2547 3981 \nL 303 3981 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-37\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <path d=\"M 50.444688 141.180247 \nL 533.780469 141.180247 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.75 -->\n      <g style=\"fill: #555555\" transform=\"translate(23.98375 144.759153)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-37\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-35\" x=\"139.013672\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <path d=\"M 50.444688 87.97939 \nL 533.780469 87.97939 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.80 -->\n      <g style=\"fill: #555555\" transform=\"translate(23.98375 91.558296)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-38\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <path d=\"M 50.444688 34.778533 \nL 533.780469 34.778533 \n\" clip-path=\"url(#p5215c33eb5)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.85 -->\n      <g style=\"fill: #555555\" transform=\"translate(23.98375 38.357439)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-38\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-35\" x=\"139.013672\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- accuracy -->\n     <g style=\"fill: #555555\" transform=\"translate(17.037188 236.659766)rotate(-90)scale(0.14 -0.14)\">\n      <defs>\n       <path id=\"Arial-BoldMT-61\" d=\"M 1116 2306 \nL 319 2450 \nQ 453 2931 781 3162 \nQ 1109 3394 1756 3394 \nQ 2344 3394 2631 3255 \nQ 2919 3116 3036 2902 \nQ 3153 2688 3153 2116 \nL 3144 1091 \nQ 3144 653 3186 445 \nQ 3228 238 3344 0 \nL 2475 0 \nQ 2441 88 2391 259 \nQ 2369 338 2359 363 \nQ 2134 144 1878 34 \nQ 1622 -75 1331 -75 \nQ 819 -75 523 203 \nQ 228 481 228 906 \nQ 228 1188 362 1408 \nQ 497 1628 739 1745 \nQ 981 1863 1438 1950 \nQ 2053 2066 2291 2166 \nL 2291 2253 \nQ 2291 2506 2166 2614 \nQ 2041 2722 1694 2722 \nQ 1459 2722 1328 2630 \nQ 1197 2538 1116 2306 \nz\nM 2291 1594 \nQ 2122 1538 1756 1459 \nQ 1391 1381 1278 1306 \nQ 1106 1184 1106 997 \nQ 1106 813 1243 678 \nQ 1381 544 1594 544 \nQ 1831 544 2047 700 \nQ 2206 819 2256 991 \nQ 2291 1103 2291 1419 \nL 2291 1594 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"Arial-BoldMT-75\" d=\"M 2644 0 \nL 2644 497 \nQ 2463 231 2167 78 \nQ 1872 -75 1544 -75 \nQ 1209 -75 943 72 \nQ 678 219 559 484 \nQ 441 750 441 1219 \nL 441 3319 \nL 1319 3319 \nL 1319 1794 \nQ 1319 1094 1367 936 \nQ 1416 778 1544 686 \nQ 1672 594 1869 594 \nQ 2094 594 2272 717 \nQ 2450 841 2515 1023 \nQ 2581 1206 2581 1919 \nL 2581 3319 \nL 3459 3319 \nL 3459 0 \nL 2644 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"Arial-BoldMT-72\" d=\"M 1300 0 \nL 422 0 \nL 422 3319 \nL 1238 3319 \nL 1238 2847 \nQ 1447 3181 1614 3287 \nQ 1781 3394 1994 3394 \nQ 2294 3394 2572 3228 \nL 2300 2463 \nQ 2078 2606 1888 2606 \nQ 1703 2606 1575 2504 \nQ 1447 2403 1373 2137 \nQ 1300 1872 1300 1025 \nL 1300 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"Arial-BoldMT-79\" d=\"M 44 3319 \nL 978 3319 \nL 1772 963 \nL 2547 3319 \nL 3456 3319 \nL 2284 125 \nL 2075 -453 \nQ 1959 -744 1854 -897 \nQ 1750 -1050 1614 -1145 \nQ 1478 -1241 1279 -1294 \nQ 1081 -1347 831 -1347 \nQ 578 -1347 334 -1294 \nL 256 -606 \nQ 463 -647 628 -647 \nQ 934 -647 1081 -467 \nQ 1228 -288 1306 -9 \nL 44 3319 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#Arial-BoldMT-61\"/>\n      <use xlink:href=\"#Arial-BoldMT-63\" x=\"55.615234\"/>\n      <use xlink:href=\"#Arial-BoldMT-63\" x=\"111.230469\"/>\n      <use xlink:href=\"#Arial-BoldMT-75\" x=\"166.845703\"/>\n      <use xlink:href=\"#Arial-BoldMT-72\" x=\"227.929688\"/>\n      <use xlink:href=\"#Arial-BoldMT-61\" x=\"266.845703\"/>\n      <use xlink:href=\"#Arial-BoldMT-63\" x=\"322.460938\"/>\n      <use xlink:href=\"#Arial-BoldMT-79\" x=\"378.076172\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <defs>\n     <path id=\"m554a502bbe\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #0000ff\"/>\n    </defs>\n    <g clip-path=\"url(#p5215c33eb5)\">\n     <use xlink:href=\"#m554a502bbe\" x=\"72.414496\" y=\"368.455746\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"103.799936\" y=\"233.993539\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"135.185376\" y=\"194.522037\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"166.570817\" y=\"169.079436\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"197.956257\" y=\"148.22644\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"229.341697\" y=\"129.988836\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"260.727138\" y=\"114.652207\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"292.112578\" y=\"101.016828\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"323.498018\" y=\"89.62856\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"354.883459\" y=\"79.827387\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"386.268899\" y=\"70.622809\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"417.654339\" y=\"61.704004\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"449.03978\" y=\"54.943393\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"480.42522\" y=\"50.239568\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"511.810661\" y=\"44.151911\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.444688 384.670937 \nL 50.444688 27.936719 \n\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 533.780469 384.670937 \nL 533.780469 27.936719 \n\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.444688 384.670937 \nL 533.780469 384.670937 \n\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.444688 27.936719 \nL 533.780469 27.936719 \n\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Training accuracy -->\n    <g style=\"fill: #262626\" transform=\"translate(228.336797 17.936719)scale(0.15 -0.15)\">\n     <defs>\n      <path id=\"Arial-BoldMT-54\" d=\"M 1497 0 \nL 1497 3806 \nL 138 3806 \nL 138 4581 \nL 3778 4581 \nL 3778 3806 \nL 2422 3806 \nL 2422 0 \nL 1497 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"Arial-BoldMT-69\" d=\"M 459 3769 \nL 459 4581 \nL 1338 4581 \nL 1338 3769 \nL 459 3769 \nz\nM 459 0 \nL 459 3319 \nL 1338 3319 \nL 1338 0 \nL 459 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"Arial-BoldMT-6e\" d=\"M 3478 0 \nL 2600 0 \nL 2600 1694 \nQ 2600 2231 2544 2389 \nQ 2488 2547 2361 2634 \nQ 2234 2722 2056 2722 \nQ 1828 2722 1647 2597 \nQ 1466 2472 1398 2265 \nQ 1331 2059 1331 1503 \nL 1331 0 \nL 453 0 \nL 453 3319 \nL 1269 3319 \nL 1269 2831 \nQ 1703 3394 2363 3394 \nQ 2653 3394 2893 3289 \nQ 3134 3184 3257 3021 \nQ 3381 2859 3429 2653 \nQ 3478 2447 3478 2063 \nL 3478 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"Arial-BoldMT-67\" d=\"M 378 -219 \nL 1381 -341 \nQ 1406 -516 1497 -581 \nQ 1622 -675 1891 -675 \nQ 2234 -675 2406 -572 \nQ 2522 -503 2581 -350 \nQ 2622 -241 2622 53 \nL 2622 538 \nQ 2228 0 1628 0 \nQ 959 0 569 566 \nQ 263 1013 263 1678 \nQ 263 2513 664 2953 \nQ 1066 3394 1663 3394 \nQ 2278 3394 2678 2853 \nL 2678 3319 \nL 3500 3319 \nL 3500 341 \nQ 3500 -247 3403 -537 \nQ 3306 -828 3131 -993 \nQ 2956 -1159 2664 -1253 \nQ 2372 -1347 1925 -1347 \nQ 1081 -1347 728 -1058 \nQ 375 -769 375 -325 \nQ 375 -281 378 -219 \nz\nM 1163 1728 \nQ 1163 1200 1367 954 \nQ 1572 709 1872 709 \nQ 2194 709 2416 961 \nQ 2638 1213 2638 1706 \nQ 2638 2222 2425 2472 \nQ 2213 2722 1888 2722 \nQ 1572 2722 1367 2476 \nQ 1163 2231 1163 1728 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"Arial-BoldMT-20\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#Arial-BoldMT-54\"/>\n     <use xlink:href=\"#Arial-BoldMT-72\" x=\"55.583984\"/>\n     <use xlink:href=\"#Arial-BoldMT-61\" x=\"94.5\"/>\n     <use xlink:href=\"#Arial-BoldMT-69\" x=\"150.115234\"/>\n     <use xlink:href=\"#Arial-BoldMT-6e\" x=\"177.898438\"/>\n     <use xlink:href=\"#Arial-BoldMT-69\" x=\"238.982422\"/>\n     <use xlink:href=\"#Arial-BoldMT-6e\" x=\"266.765625\"/>\n     <use xlink:href=\"#Arial-BoldMT-67\" x=\"327.849609\"/>\n     <use xlink:href=\"#Arial-BoldMT-20\" x=\"388.933594\"/>\n     <use xlink:href=\"#Arial-BoldMT-61\" x=\"416.716797\"/>\n     <use xlink:href=\"#Arial-BoldMT-63\" x=\"472.332031\"/>\n     <use xlink:href=\"#Arial-BoldMT-63\" x=\"527.947266\"/>\n     <use xlink:href=\"#Arial-BoldMT-75\" x=\"583.5625\"/>\n     <use xlink:href=\"#Arial-BoldMT-72\" x=\"644.646484\"/>\n     <use xlink:href=\"#Arial-BoldMT-61\" x=\"683.5625\"/>\n     <use xlink:href=\"#Arial-BoldMT-63\" x=\"739.177734\"/>\n     <use xlink:href=\"#Arial-BoldMT-79\" x=\"794.792969\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 57.444688 50.199219 \nL 143.53375 50.199219 \nQ 145.53375 50.199219 145.53375 48.199219 \nL 145.53375 34.936719 \nQ 145.53375 32.936719 143.53375 32.936719 \nL 57.444688 32.936719 \nQ 55.444688 32.936719 55.444688 34.936719 \nL 55.444688 48.199219 \nQ 55.444688 50.199219 57.444688 50.199219 \nz\n\" style=\"fill: #e5e5e5; opacity: 0.8; stroke: #cccccc; stroke-width: 0.5; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <g>\n      <use xlink:href=\"#m554a502bbe\" x=\"69.444688\" y=\"40.594531\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- Training acc -->\n     <g style=\"fill: #262626\" transform=\"translate(87.444688 44.094531)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"ArialMT-54\" d=\"M 1659 0 \nL 1659 4041 \nL 150 4041 \nL 150 4581 \nL 3781 4581 \nL 3781 4041 \nL 2266 4041 \nL 2266 0 \nL 1659 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-72\" d=\"M 416 0 \nL 416 3319 \nL 922 3319 \nL 922 2816 \nQ 1116 3169 1280 3281 \nQ 1444 3394 1641 3394 \nQ 1925 3394 2219 3213 \nL 2025 2691 \nQ 1819 2813 1613 2813 \nQ 1428 2813 1281 2702 \nQ 1134 2591 1072 2394 \nQ 978 2094 978 1738 \nL 978 0 \nL 416 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-61\" d=\"M 2588 409 \nQ 2275 144 1986 34 \nQ 1697 -75 1366 -75 \nQ 819 -75 525 192 \nQ 231 459 231 875 \nQ 231 1119 342 1320 \nQ 453 1522 633 1644 \nQ 813 1766 1038 1828 \nQ 1203 1872 1538 1913 \nQ 2219 1994 2541 2106 \nQ 2544 2222 2544 2253 \nQ 2544 2597 2384 2738 \nQ 2169 2928 1744 2928 \nQ 1347 2928 1158 2789 \nQ 969 2650 878 2297 \nL 328 2372 \nQ 403 2725 575 2942 \nQ 747 3159 1072 3276 \nQ 1397 3394 1825 3394 \nQ 2250 3394 2515 3294 \nQ 2781 3194 2906 3042 \nQ 3031 2891 3081 2659 \nQ 3109 2516 3109 2141 \nL 3109 1391 \nQ 3109 606 3145 398 \nQ 3181 191 3288 0 \nL 2700 0 \nQ 2613 175 2588 409 \nz\nM 2541 1666 \nQ 2234 1541 1622 1453 \nQ 1275 1403 1131 1340 \nQ 988 1278 909 1158 \nQ 831 1038 831 891 \nQ 831 666 1001 516 \nQ 1172 366 1500 366 \nQ 1825 366 2078 508 \nQ 2331 650 2450 897 \nQ 2541 1088 2541 1459 \nL 2541 1666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-69\" d=\"M 425 3934 \nL 425 4581 \nL 988 4581 \nL 988 3934 \nL 425 3934 \nz\nM 425 0 \nL 425 3319 \nL 988 3319 \nL 988 0 \nL 425 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-6e\" d=\"M 422 0 \nL 422 3319 \nL 928 3319 \nL 928 2847 \nQ 1294 3394 1984 3394 \nQ 2284 3394 2536 3286 \nQ 2788 3178 2913 3003 \nQ 3038 2828 3088 2588 \nQ 3119 2431 3119 2041 \nL 3119 0 \nL 2556 0 \nL 2556 2019 \nQ 2556 2363 2490 2533 \nQ 2425 2703 2258 2804 \nQ 2091 2906 1866 2906 \nQ 1506 2906 1245 2678 \nQ 984 2450 984 1813 \nL 984 0 \nL 422 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-67\" d=\"M 319 -275 \nL 866 -356 \nQ 900 -609 1056 -725 \nQ 1266 -881 1628 -881 \nQ 2019 -881 2231 -725 \nQ 2444 -569 2519 -288 \nQ 2563 -116 2559 434 \nQ 2191 0 1641 0 \nQ 956 0 581 494 \nQ 206 988 206 1678 \nQ 206 2153 378 2554 \nQ 550 2956 876 3175 \nQ 1203 3394 1644 3394 \nQ 2231 3394 2613 2919 \nL 2613 3319 \nL 3131 3319 \nL 3131 450 \nQ 3131 -325 2973 -648 \nQ 2816 -972 2473 -1159 \nQ 2131 -1347 1631 -1347 \nQ 1038 -1347 672 -1080 \nQ 306 -813 319 -275 \nz\nM 784 1719 \nQ 784 1066 1043 766 \nQ 1303 466 1694 466 \nQ 2081 466 2343 764 \nQ 2606 1063 2606 1700 \nQ 2606 2309 2336 2618 \nQ 2066 2928 1684 2928 \nQ 1309 2928 1046 2623 \nQ 784 2319 784 1719 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-63\" d=\"M 2588 1216 \nL 3141 1144 \nQ 3050 572 2676 248 \nQ 2303 -75 1759 -75 \nQ 1078 -75 664 370 \nQ 250 816 250 1647 \nQ 250 2184 428 2587 \nQ 606 2991 970 3192 \nQ 1334 3394 1763 3394 \nQ 2303 3394 2647 3120 \nQ 2991 2847 3088 2344 \nL 2541 2259 \nQ 2463 2594 2264 2762 \nQ 2066 2931 1784 2931 \nQ 1359 2931 1093 2626 \nQ 828 2322 828 1663 \nQ 828 994 1084 691 \nQ 1341 388 1753 388 \nQ 2084 388 2306 591 \nQ 2528 794 2588 1216 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-54\"/>\n      <use xlink:href=\"#ArialMT-72\" x=\"57.333984\"/>\n      <use xlink:href=\"#ArialMT-61\" x=\"90.634766\"/>\n      <use xlink:href=\"#ArialMT-69\" x=\"146.25\"/>\n      <use xlink:href=\"#ArialMT-6e\" x=\"168.466797\"/>\n      <use xlink:href=\"#ArialMT-69\" x=\"224.082031\"/>\n      <use xlink:href=\"#ArialMT-6e\" x=\"246.298828\"/>\n      <use xlink:href=\"#ArialMT-67\" x=\"301.914062\"/>\n      <use xlink:href=\"#ArialMT-20\" x=\"357.529297\"/>\n      <use xlink:href=\"#ArialMT-61\" x=\"385.3125\"/>\n      <use xlink:href=\"#ArialMT-63\" x=\"440.927734\"/>\n      <use xlink:href=\"#ArialMT-63\" x=\"490.927734\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 582.264219 384.670937 \nL 1065.6 384.670937 \nL 1065.6 27.936719 \nL 582.264219 27.936719 \nz\n\" style=\"fill: #e5e5e5\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_8\">\n     <g id=\"line2d_17\">\n      <path d=\"M 635.619467 384.670937 \nL 635.619467 27.936719 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_19\">\n      <!-- 2 -->\n      <g style=\"fill: #555555\" transform=\"translate(632.838999 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_18\">\n      <path d=\"M 698.390348 384.670937 \nL 698.390348 27.936719 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_20\">\n      <!-- 4 -->\n      <g style=\"fill: #555555\" transform=\"translate(695.609879 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path d=\"M 761.161229 384.670937 \nL 761.161229 27.936719 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_21\">\n      <!-- 6 -->\n      <g style=\"fill: #555555\" transform=\"translate(758.38076 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_20\">\n      <path d=\"M 823.932109 384.670937 \nL 823.932109 27.936719 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_22\">\n      <!-- 8 -->\n      <g style=\"fill: #555555\" transform=\"translate(821.151641 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_21\">\n      <path d=\"M 886.70299 384.670937 \nL 886.70299 27.936719 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_23\">\n      <!-- 10 -->\n      <g style=\"fill: #555555\" transform=\"translate(881.142053 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_22\">\n      <path d=\"M 949.473871 384.670937 \nL 949.473871 27.936719 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_24\">\n      <!-- 12 -->\n      <g style=\"fill: #555555\" transform=\"translate(943.912933 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-32\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_23\">\n      <path d=\"M 1012.244751 384.670937 \nL 1012.244751 27.936719 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_25\">\n      <!-- 14 -->\n      <g style=\"fill: #555555\" transform=\"translate(1006.683814 398.82875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-34\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_26\">\n     <!-- epochs -->\n     <g style=\"fill: #555555\" transform=\"translate(799.427734 414.837187)scale(0.14 -0.14)\">\n      <use xlink:href=\"#Arial-BoldMT-65\"/>\n      <use xlink:href=\"#Arial-BoldMT-70\" x=\"55.615234\"/>\n      <use xlink:href=\"#Arial-BoldMT-6f\" x=\"116.699219\"/>\n      <use xlink:href=\"#Arial-BoldMT-63\" x=\"177.783203\"/>\n      <use xlink:href=\"#Arial-BoldMT-68\" x=\"233.398438\"/>\n      <use xlink:href=\"#Arial-BoldMT-73\" x=\"294.482422\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_8\">\n     <g id=\"line2d_24\">\n      <path d=\"M 582.264219 365.708065 \nL 1065.6 365.708065 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_27\">\n      <!-- 0.6 -->\n      <g style=\"fill: #555555\" transform=\"translate(561.364219 369.286971)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-36\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_25\">\n      <path d=\"M 582.264219 306.413891 \nL 1065.6 306.413891 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0.8 -->\n      <g style=\"fill: #555555\" transform=\"translate(561.364219 309.992798)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-38\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_26\">\n      <path d=\"M 582.264219 247.119718 \nL 1065.6 247.119718 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_29\">\n      <!-- 1.0 -->\n      <g style=\"fill: #555555\" transform=\"translate(561.364219 250.698624)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_27\">\n      <path d=\"M 582.264219 187.825544 \nL 1065.6 187.825544 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_30\">\n      <!-- 1.2 -->\n      <g style=\"fill: #555555\" transform=\"translate(561.364219 191.40445)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-32\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_28\">\n      <path d=\"M 582.264219 128.531371 \nL 1065.6 128.531371 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_31\">\n      <!-- 1.4 -->\n      <g style=\"fill: #555555\" transform=\"translate(561.364219 132.110277)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-34\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_29\">\n      <path d=\"M 582.264219 69.237197 \nL 1065.6 69.237197 \n\" clip-path=\"url(#pb3fa4a8e1a)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_32\">\n      <!-- 1.6 -->\n      <g style=\"fill: #555555\" transform=\"translate(561.364219 72.816103)scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-36\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_33\">\n     <!-- loss -->\n     <g style=\"fill: #555555\" transform=\"translate(554.601406 220.309297)rotate(-90)scale(0.14 -0.14)\">\n      <defs>\n       <path id=\"Arial-BoldMT-6c\" d=\"M 459 0 \nL 459 4581 \nL 1338 4581 \nL 1338 0 \nL 459 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#Arial-BoldMT-6c\"/>\n      <use xlink:href=\"#Arial-BoldMT-6f\" x=\"27.783203\"/>\n      <use xlink:href=\"#Arial-BoldMT-73\" x=\"88.867188\"/>\n      <use xlink:href=\"#Arial-BoldMT-73\" x=\"144.482422\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_30\">\n    <g clip-path=\"url(#pb3fa4a8e1a)\">\n     <use xlink:href=\"#m554a502bbe\" x=\"604.234027\" y=\"44.151911\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"635.619467\" y=\"209.947058\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"667.004908\" y=\"242.39639\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"698.390348\" y=\"264.079959\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"729.775788\" y=\"282.063618\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"761.161229\" y=\"297.036255\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"792.546669\" y=\"309.633516\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"823.932109\" y=\"320.304793\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"855.31755\" y=\"330.076737\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"886.70299\" y=\"338.405894\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"918.08843\" y=\"345.891293\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"949.473871\" y=\"352.502737\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"980.859311\" y=\"358.235417\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"1012.244751\" y=\"363.787233\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     <use xlink:href=\"#m554a502bbe\" x=\"1043.630192\" y=\"368.455746\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n    </g>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 582.264219 384.670937 \nL 582.264219 27.936719 \n\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 1065.6 384.670937 \nL 1065.6 27.936719 \n\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 582.264219 384.670937 \nL 1065.6 384.670937 \n\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 582.264219 27.936719 \nL 1065.6 27.936719 \n\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_34\">\n    <!-- Training loss -->\n    <g style=\"fill: #262626\" transform=\"translate(777.674688 17.936719)scale(0.15 -0.15)\">\n     <use xlink:href=\"#Arial-BoldMT-54\"/>\n     <use xlink:href=\"#Arial-BoldMT-72\" x=\"55.583984\"/>\n     <use xlink:href=\"#Arial-BoldMT-61\" x=\"94.5\"/>\n     <use xlink:href=\"#Arial-BoldMT-69\" x=\"150.115234\"/>\n     <use xlink:href=\"#Arial-BoldMT-6e\" x=\"177.898438\"/>\n     <use xlink:href=\"#Arial-BoldMT-69\" x=\"238.982422\"/>\n     <use xlink:href=\"#Arial-BoldMT-6e\" x=\"266.765625\"/>\n     <use xlink:href=\"#Arial-BoldMT-67\" x=\"327.849609\"/>\n     <use xlink:href=\"#Arial-BoldMT-20\" x=\"388.933594\"/>\n     <use xlink:href=\"#Arial-BoldMT-6c\" x=\"416.716797\"/>\n     <use xlink:href=\"#Arial-BoldMT-6f\" x=\"444.5\"/>\n     <use xlink:href=\"#Arial-BoldMT-73\" x=\"505.583984\"/>\n     <use xlink:href=\"#Arial-BoldMT-73\" x=\"561.199219\"/>\n    </g>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_13\">\n     <path d=\"M 970.289062 50.199219 \nL 1058.6 50.199219 \nQ 1060.6 50.199219 1060.6 48.199219 \nL 1060.6 34.936719 \nQ 1060.6 32.936719 1058.6 32.936719 \nL 970.289062 32.936719 \nQ 968.289062 32.936719 968.289062 34.936719 \nL 968.289062 48.199219 \nQ 968.289062 50.199219 970.289062 50.199219 \nz\n\" style=\"fill: #e5e5e5; opacity: 0.8; stroke: #cccccc; stroke-width: 0.5; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_31\">\n     <g>\n      <use xlink:href=\"#m554a502bbe\" x=\"982.289062\" y=\"40.594531\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n     </g>\n    </g>\n    <g id=\"text_35\">\n     <!-- Training loss -->\n     <g style=\"fill: #262626\" transform=\"translate(1000.289062 44.094531)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"ArialMT-6c\" d=\"M 409 0 \nL 409 4581 \nL 972 4581 \nL 972 0 \nL 409 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-6f\" d=\"M 213 1659 \nQ 213 2581 725 3025 \nQ 1153 3394 1769 3394 \nQ 2453 3394 2887 2945 \nQ 3322 2497 3322 1706 \nQ 3322 1066 3130 698 \nQ 2938 331 2570 128 \nQ 2203 -75 1769 -75 \nQ 1072 -75 642 372 \nQ 213 819 213 1659 \nz\nM 791 1659 \nQ 791 1022 1069 705 \nQ 1347 388 1769 388 \nQ 2188 388 2466 706 \nQ 2744 1025 2744 1678 \nQ 2744 2294 2464 2611 \nQ 2184 2928 1769 2928 \nQ 1347 2928 1069 2612 \nQ 791 2297 791 1659 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-73\" d=\"M 197 991 \nL 753 1078 \nQ 800 744 1014 566 \nQ 1228 388 1613 388 \nQ 2000 388 2187 545 \nQ 2375 703 2375 916 \nQ 2375 1106 2209 1216 \nQ 2094 1291 1634 1406 \nQ 1016 1563 777 1677 \nQ 538 1791 414 1992 \nQ 291 2194 291 2438 \nQ 291 2659 392 2848 \nQ 494 3038 669 3163 \nQ 800 3259 1026 3326 \nQ 1253 3394 1513 3394 \nQ 1903 3394 2198 3281 \nQ 2494 3169 2634 2976 \nQ 2775 2784 2828 2463 \nL 2278 2388 \nQ 2241 2644 2061 2787 \nQ 1881 2931 1553 2931 \nQ 1166 2931 1000 2803 \nQ 834 2675 834 2503 \nQ 834 2394 903 2306 \nQ 972 2216 1119 2156 \nQ 1203 2125 1616 2013 \nQ 2213 1853 2448 1751 \nQ 2684 1650 2818 1456 \nQ 2953 1263 2953 975 \nQ 2953 694 2789 445 \nQ 2625 197 2315 61 \nQ 2006 -75 1616 -75 \nQ 969 -75 630 194 \nQ 291 463 197 991 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-54\"/>\n      <use xlink:href=\"#ArialMT-72\" x=\"57.333984\"/>\n      <use xlink:href=\"#ArialMT-61\" x=\"90.634766\"/>\n      <use xlink:href=\"#ArialMT-69\" x=\"146.25\"/>\n      <use xlink:href=\"#ArialMT-6e\" x=\"168.466797\"/>\n      <use xlink:href=\"#ArialMT-69\" x=\"224.082031\"/>\n      <use xlink:href=\"#ArialMT-6e\" x=\"246.298828\"/>\n      <use xlink:href=\"#ArialMT-67\" x=\"301.914062\"/>\n      <use xlink:href=\"#ArialMT-20\" x=\"357.529297\"/>\n      <use xlink:href=\"#ArialMT-6c\" x=\"385.3125\"/>\n      <use xlink:href=\"#ArialMT-6f\" x=\"407.529297\"/>\n      <use xlink:href=\"#ArialMT-73\" x=\"463.144531\"/>\n      <use xlink:href=\"#ArialMT-73\" x=\"513.144531\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5215c33eb5\">\n   <rect x=\"50.444688\" y=\"27.936719\" width=\"483.335781\" height=\"356.734219\"/>\n  </clipPath>\n  <clipPath id=\"pb3fa4a8e1a\">\n   <rect x=\"582.264219\" y=\"27.936719\" width=\"483.335781\" height=\"356.734219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data:  (167616,)\n",
      "shape of target variable:  (167616,)\n",
      "Length of word index: 53645\n"
     ]
    }
   ],
   "source": [
    "# ORDER 2 TRAIN DATA\n",
    "X_train = X2_train\n",
    "Y_train = Y2_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=9)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 20, 100)           5364500   \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 20, 128)          84480     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 20, 128)          98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 20, 128)          24704     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 18, 72)            27720     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 9, 72)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 9, 64)             8768      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,634,533\n",
      "Trainable params: 5,634,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(9, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1310/1310 [==============================] - 358s 251ms/step - loss: 1.7094 - accuracy: 0.5286\n",
      "Epoch 2/15\n",
      "1310/1310 [==============================] - 343s 262ms/step - loss: 1.1475 - accuracy: 0.6568\n",
      "Epoch 3/15\n",
      "1310/1310 [==============================] - 333s 254ms/step - loss: 1.0230 - accuracy: 0.6970\n",
      "Epoch 4/15\n",
      " 610/1310 [============>.................] - ETA: 3:08 - loss: 0.9365 - accuracy: 0.7243"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\natal\\CSCI5922_NN\\FinalProject\\Semantical_Clustering_Data_Classification_Model\\scripts\\models_model2_headlines_superclassesOK.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natal/CSCI5922_NN/FinalProject/Semantical_Clustering_Data_Classification_Model/scripts/models_model2_headlines_superclassesOK.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m callback_list \u001b[39m=\u001b[39m [checkpointer, earlystopping]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natal/CSCI5922_NN/FinalProject/Semantical_Clustering_Data_Classification_Model/scripts/models_model2_headlines_superclassesOK.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# fit model to the data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/natal/CSCI5922_NN/FinalProject/Semantical_Clustering_Data_Classification_Model/scripts/models_model2_headlines_superclassesOK.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m history2 \u001b[39m=\u001b[39m model2\u001b[39m.\u001b[39;49mfit(train_padseq, Y_train, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natal/CSCI5922_NN/FinalProject/Semantical_Clustering_Data_Classification_Model/scripts/models_model2_headlines_superclassesOK.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                      batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natal/CSCI5922_NN/FinalProject/Semantical_Clustering_Data_Classification_Model/scripts/models_model2_headlines_superclassesOK.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                      epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natal/CSCI5922_NN/FinalProject/Semantical_Clustering_Data_Classification_Model/scripts/models_model2_headlines_superclassesOK.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natal/CSCI5922_NN/FinalProject/Semantical_Clustering_Data_Classification_Model/scripts/models_model2_headlines_superclassesOK.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# evalute the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natal/CSCI5922_NN/FinalProject/Semantical_Clustering_Data_Classification_Model/scripts/models_model2_headlines_superclassesOK.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m test_loss2, test_acc2 \u001b[39m=\u001b[39m model2\u001b[39m.\u001b[39mevaluate(test_padseq, Y_test, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1212\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1213\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1214\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1215\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1216\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1217\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1218\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    907\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 910\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    912\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    913\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    939\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    940\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    941\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    945\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3127\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3128\u001b[0m   (graph_function,\n\u001b[0;32m   3129\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3131\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1955\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1956\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1957\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1958\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1960\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1961\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m     args,\n\u001b[0;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1964\u001b[0m     executing_eagerly)\n\u001b[0;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    597\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    601\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    602\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    604\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    606\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    607\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    610\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    611\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, Y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, Y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 3 TRAIN DATA\n",
    "X_train = X3_train\n",
    "Y_train = Y3_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=9)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(9, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 4 TRAIN DATA\n",
    "X_train = X4_train\n",
    "Y_train = Y4_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=9)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(9, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 5 TRAIN DATA\n",
    "X_train = X5_train\n",
    "Y_train = Y5_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=9)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(9, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85b31755cbf75356c393a3522367cd288f0b05170e2bd292c75b11fc3d2da2cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
