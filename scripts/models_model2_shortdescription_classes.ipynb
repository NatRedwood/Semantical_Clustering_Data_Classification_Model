{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics as st\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras\n",
    "from keras import layers, Input, Model\n",
    "from keras.layers import GRU, SimpleRNN, Embedding, Dense, LSTM, Dropout, TextVectorization\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib, import_ipynb\n",
    "import data_clean_order_text as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ordered_super_alpha_text\n",
    "%store -r ordered_class_alpha_text\n",
    "%store -r ordered_sem_clusters_desc_text\n",
    "%store -r ordered_sem_clusters_asc_text\n",
    "%store -r ordered_sem_clusters_shuffled_per_superclass_text\n",
    "%store -r test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental data orderings on headlines TRAIN EXAMPLES\n",
    "X1_train = ordered_super_alpha_text['cleaned_headline']\n",
    "X2_train = ordered_class_alpha_text['cleaned_headline']\n",
    "X3_train = ordered_sem_clusters_desc_text['cleaned_headline']\n",
    "X4_train = ordered_sem_clusters_asc_text['cleaned_headline']\n",
    "X5_train = ordered_sem_clusters_shuffled_per_superclass_text['cleaned_headline']\n",
    "\n",
    "# Experimental data orderings on short_description #TODO\n",
    "\n",
    "# TRAIN LABELS\n",
    "Y1_train = ordered_super_alpha_text['class']\n",
    "Y2_train = ordered_class_alpha_text['class']\n",
    "Y3_train = ordered_sem_clusters_desc_text['class']\n",
    "Y4_train = ordered_sem_clusters_asc_text['class']\n",
    "Y5_train = ordered_sem_clusters_shuffled_per_superclass_text['class']\n",
    "\n",
    "# TEST EXAMPLES\n",
    "X_test = test_df['cleaned_headline']\n",
    "Y_test = test_df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### One-hot encoding and indexing of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_test = encoder.fit_transform(Y_test)\n",
    "print(\"shape of input data: \", X_test.shape)\n",
    "print(\"shape of target variable: \", Y_test.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_test) # build the word index\n",
    "# padding X_test text input data\n",
    "test_seq = tokenizer.texts_to_sequences(X_test) # converts strinfs into integer lists\n",
    "test_padseq = pad_sequences(test_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 150000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index)\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_test = to_categorical(Y_test, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 1 TRAIN DATA\n",
    "X_train = X1_train\n",
    "Y_train = Y1_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(41, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 2 TRAIN DATA\n",
    "X_train = X2_train\n",
    "Y_train = Y2_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(41, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 3 TRAIN DATA\n",
    "X_train = X3_train\n",
    "Y_train = Y3_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(41, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 4 TRAIN DATA\n",
    "X_train = X4_train\n",
    "Y_train = Y4_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(41, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 5 TRAIN DATA\n",
    "X_train = X5_train\n",
    "Y_train = Y5_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2, training using Conv1D, Bi-directional RNN, LSTMs and GRU layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(41, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85b31755cbf75356c393a3522367cd288f0b05170e2bd292c75b11fc3d2da2cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
