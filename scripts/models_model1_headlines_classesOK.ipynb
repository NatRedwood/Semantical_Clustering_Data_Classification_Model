{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data_clean_order_text.ipynb\n",
      "Length of original data set 209527\n",
      "Number of examples with no short description:  19712\n",
      "Number of examples with no headline:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natal\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "c:\\Users\\natal\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "c:\\Users\\natal\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "c:\\Users\\natal\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\natal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "<string>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<string>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_df' (DataFrame)\n",
      "Size of training data for headlines:  167616\n",
      "Size of testing data for headlines:  41905\n",
      "Size of training data for headlines:  151851\n",
      "Size of testing data for headlines:  37963\n",
      "Stored 'ordered_super_alpha_text' (DataFrame)\n",
      "Stored 'ordered_class_alpha_text' (DataFrame)\n",
      "Stored 'ordered_sem_clusters_desc_text' (DataFrame)\n",
      "Stored 'ordered_sem_clusters_asc_text' (DataFrame)\n",
      "Stored 'ordered_sem_clusters_shuffled_per_superclass_text' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import importlib, import_ipynb\n",
    "import data_clean_order_text as data\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import experimental\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# model building imports\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# matplotlib defaults\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ordered data from data_clean_order_text.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ordered_super_alpha_text\n",
    "%store -r ordered_class_alpha_text\n",
    "%store -r ordered_sem_clusters_desc_text\n",
    "%store -r ordered_sem_clusters_asc_text\n",
    "%store -r ordered_sem_clusters_shuffled_per_superclass_text\n",
    "%store -r test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental data orderings on headlines TRAIN EXAMPLES\n",
    "X1_train = ordered_super_alpha_text['cleaned_headline']\n",
    "X2_train = ordered_class_alpha_text['cleaned_headline']\n",
    "X3_train = ordered_sem_clusters_desc_text['cleaned_headline']\n",
    "X4_train = ordered_sem_clusters_asc_text['cleaned_headline']\n",
    "X5_train = ordered_sem_clusters_shuffled_per_superclass_text['cleaned_headline']\n",
    "\n",
    "# Experimental data orderings on short_description #TODO\n",
    "\n",
    "# TRAIN LABELS\n",
    "Y1_train = ordered_super_alpha_text['class']\n",
    "Y2_train = ordered_class_alpha_text['class']\n",
    "Y3_train = ordered_sem_clusters_desc_text['class']\n",
    "Y4_train = ordered_sem_clusters_asc_text['class']\n",
    "Y5_train = ordered_sem_clusters_shuffled_per_superclass_text['class']\n",
    "\n",
    "# TEST EXAMPLES\n",
    "X_test = test_df['cleaned_headline']\n",
    "Y_test = test_df['class']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      QUEER VOICES\n",
       "1             WOMEN\n",
       "2             WOMEN\n",
       "3      BLACK VOICES\n",
       "4      QUEER VOICES\n",
       "          ...      \n",
       "95     QUEER VOICES\n",
       "96     QUEER VOICES\n",
       "97    LATINO VOICES\n",
       "98     QUEER VOICES\n",
       "99    LATINO VOICES\n",
       "Name: class, Length: 100, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ARTS\n",
       "1     ARTS\n",
       "2     ARTS\n",
       "3     ARTS\n",
       "4     ARTS\n",
       "      ... \n",
       "95    ARTS\n",
       "96    ARTS\n",
       "97    ARTS\n",
       "98    ARTS\n",
       "99    ARTS\n",
       "Name: class, Length: 100, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89054     GENERAL POLITICS\n",
       "36891     GENERAL POLITICS\n",
       "72404     GENERAL POLITICS\n",
       "6859      GENERAL POLITICS\n",
       "27189     GENERAL POLITICS\n",
       "                ...       \n",
       "101652    GENERAL POLITICS\n",
       "68000     GENERAL POLITICS\n",
       "69019     GENERAL POLITICS\n",
       "8237      GENERAL POLITICS\n",
       "65266     GENERAL POLITICS\n",
       "Name: class, Length: 100, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y3_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209514    CULTURE & ARTS\n",
       "168362    CULTURE & ARTS\n",
       "206956    CULTURE & ARTS\n",
       "171094    CULTURE & ARTS\n",
       "138041    CULTURE & ARTS\n",
       "               ...      \n",
       "170438    CULTURE & ARTS\n",
       "174433    CULTURE & ARTS\n",
       "175788    CULTURE & ARTS\n",
       "167105    CULTURE & ARTS\n",
       "157167    CULTURE & ARTS\n",
       "Name: class, Length: 100, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y4_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35183    THE WORLDPOST\n",
       "45840    THE WORLDPOST\n",
       "57787    THE WORLDPOST\n",
       "62535    THE WORLDPOST\n",
       "44432    THE WORLDPOST\n",
       "             ...      \n",
       "46220    THE WORLDPOST\n",
       "47155    THE WORLDPOST\n",
       "62475    THE WORLDPOST\n",
       "57798    THE WORLDPOST\n",
       "51993    THE WORLDPOST\n",
       "Name: class, Length: 100, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y5_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100952     QUEER VOICES\n",
       "156276           TRAVEL\n",
       "129361            CRIME\n",
       "157804     QUEER VOICES\n",
       "185209           TRAVEL\n",
       "              ...      \n",
       "116817             ARTS\n",
       "69235           LEISURE\n",
       "113929          PARENTS\n",
       "144547          LEISURE\n",
       "143945    HOME & LIVING\n",
       "Name: class, Length: 100, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### One-hot encoding and indexing of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data:  (41905,)\n",
      "shape of target variable:  (41905,)\n",
      "Length of word index: 27409\n"
     ]
    }
   ],
   "source": [
    "# TEST DATA\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_test = encoder.fit_transform(Y_test)\n",
    "print(\"shape of input data: \", X_test.shape)\n",
    "print(\"shape of target variable: \", Y_test.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_test) # build the word index\n",
    "# padding X_test text input data\n",
    "test_seq = tokenizer.texts_to_sequences(X_test) # converts strinfs into integer lists\n",
    "test_padseq = pad_sequences(test_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 150000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index)\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_test = to_categorical(Y_test, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data:  (167616,)\n",
      "shape of target variable:  (167616,)\n",
      "Length of word index: 53538\n"
     ]
    }
   ],
   "source": [
    "# ORDER 1 TRAIN DATA\n",
    "X_train = X1_train\n",
    "Y_train = Y1_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 1000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training using embedding layer and RNN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, 20, 70)            3747660   \n",
      "                                                                 \n",
      " bidirectional_18 (Bidirecti  (None, 20, 128)          17280     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_19 (Bidirecti  (None, 20, 128)          24704     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " simple_rnn_33 (SimpleRNN)   (None, 32)                5152      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 42)                1386      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,796,182\n",
      "Trainable params: 3,796,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# basline model using embedding layers and simpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 70, input_length=maxlen))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))\n",
    "model.add(SimpleRNN(32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(42, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1310/1310 [==============================] - 102s 72ms/step - loss: 2.7662 - accuracy: 0.3069\n",
      "Epoch 2/15\n",
      "1310/1310 [==============================] - 105s 80ms/step - loss: 2.1611 - accuracy: 0.4439\n",
      "Epoch 3/15\n",
      "1310/1310 [==============================] - 110s 84ms/step - loss: 1.9394 - accuracy: 0.4953\n",
      "Epoch 4/15\n",
      " 679/1310 [==============>...............] - ETA: 49s - loss: 1.8127 - accuracy: 0.5254"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "#SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                             patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                             )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "callback_list = [earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history = model.fit(train_padseq, Y_train, \n",
    "                   batch_size=128, \n",
    "                    epochs=15\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute the model\n",
    "test_loss, test_acc = model.evaluate(test_padseq, Y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "\n",
    "    # create object of arrays of accuracy and loss\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    # number of epochs in our model\n",
    "    epochs = range(1 ,len(acc) + 1)\n",
    "    \n",
    "    # call matplolib figure object and plot loss and accuracy curves\n",
    "    plt.figure(figsize=(15,6))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title(\"Training and validation accuracy\", fontsize=15)\n",
    "    plt.xlabel('epochs', fontsize=14)\n",
    "    plt.ylabel(\"accuracy\", fontsize=14)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title(\"Training and validation loss\", fontsize=15)\n",
    "    plt.xlabel('epochs', fontsize=14)\n",
    "    plt.ylabel(\"loss\", fontsize=14)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 2 TRAIN DATA\n",
    "X_train = X2_train\n",
    "Y_train = Y2_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 5000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training using embedding layer and RNN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basline model using embedding layers and simpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 70, input_length=maxlen))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))\n",
    "model.add(SimpleRNN(32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(42, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "#SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                             patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                             )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "callback_list = [earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history = model.fit(train_padseq, Y_train, \n",
    "                   batch_size=128, \n",
    "                    epochs=15\n",
    "                   )\n",
    "\n",
    "# evalute the model\n",
    "test_loss, test_acc = model.evaluate(test_padseq, Y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 3 TRAIN DATA\n",
    "X_train = X3_train\n",
    "Y_train = Y3_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 5000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training using embedding layer and RNN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basline model using embedding layers and simpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 70, input_length=maxlen))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))\n",
    "model.add(SimpleRNN(32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(42, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "#SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                             patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                             )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "callback_list = [earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history = model.fit(train_padseq, Y_train, \n",
    "                   batch_size=128, \n",
    "                    epochs=15\n",
    "                   )\n",
    "\n",
    "# evalute the model\n",
    "test_loss, test_acc = model.evaluate(test_padseq, Y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 4 TRAIN DATA\n",
    "X_train = X4_train\n",
    "Y_train = Y4_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 5000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training using embedding layer and RNN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basline model using embedding layers and simpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 70, input_length=maxlen))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))\n",
    "model.add(SimpleRNN(32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(42, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "#SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                             patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                             )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "callback_list = [earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history = model.fit(train_padseq, Y_train, \n",
    "                   batch_size=128, \n",
    "                    epochs=15\n",
    "                   )\n",
    "\n",
    "# evalute the model\n",
    "test_loss, test_acc = model.evaluate(test_padseq, Y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT ORDER 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER 5 TRAIN DATA\n",
    "X_train = X5_train\n",
    "Y_train = Y5_train\n",
    "\n",
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"shape of input data: \", X_train.shape)\n",
    "print(\"shape of target variable: \", Y_train.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=20) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 15000000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index) + 5000\n",
    "maxlen = 20 # max length of sequence \n",
    "Y_train = to_categorical(Y_train, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training using embedding layer and RNN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basline model using embedding layers and simpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 70, input_length=maxlen))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))\n",
    "model.add(SimpleRNN(32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(42, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "#SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                             patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                             )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "callback_list = [earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history = model.fit(train_padseq, Y_train, \n",
    "                   batch_size=128, \n",
    "                    epochs=15\n",
    "                   )\n",
    "\n",
    "# evalute the model\n",
    "test_loss, test_acc = model.evaluate(test_padseq, Y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85b31755cbf75356c393a3522367cd288f0b05170e2bd292c75b11fc3d2da2cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
